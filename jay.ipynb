{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xj_iapqC7ky"
      },
      "outputs": [],
      "source": [
        "pip install mlxtend\n",
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "transactions = [['bread', 'milk'],\n",
        "['bread', 'diaper','beer','eggs'],\n",
        "['milk','diaper','beer','coke'],\n",
        "['bread', 'milk','diaper','beer'],\n",
        "['bread','milk','diaper','coke']]\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "te=TransactionEncoder()\n",
        "te_array=te.fit(transactions).transform(transactions)\n",
        "df=pd.DataFrame(te_array, columns=te.columns_)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apriori Algorithm\n",
        "freq_items = apriori(df, min_support = 0.5, use_colnames = True)\n",
        "print(freq_items)"
      ],
      "metadata": {
        "id": "fAEdQgGCC8qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mining Association Rules\n",
        "rules = association_rules(freq_items, metric ='confidence',min_threshold=0.05)\n",
        "rules = rules.sort_values(['support', 'confidence'], ascending=[False,False])\n",
        "rules\n",
        "#Interpretation\n",
        "#Change the min_support value and perform again"
      ],
      "metadata": {
        "id": "f1IPArBHDBIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If data is to be taken from csv file\n",
        "import pandas as pd\n",
        "data=pd.read_csv('Market_Basket_Optimisation.csv')\n",
        "# Getting the list of transactions from the dataset\n",
        "transactions = []\n",
        "for i in range(0, len(data)):\n",
        "transactions.append([str(data.values[i,j]) for j in range(0,len(data.columns))])\n",
        "\n",
        "\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "te=TransactionEncoder()\n",
        "te_array=te.fit(transactions).transform(transactions)\n",
        "df=pd.DataFrame(te_array, columns=te.columns_)\n",
        "df"
      ],
      "metadata": {
        "id": "QyYzYf89DGsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Apriori algorithm on the dataset\n",
        "!pip install apyori\n",
        "from apyori import apriori\n",
        "\n",
        "\n",
        "model=apriori(transactions, min_support = 0.003, min_confidence = 0.2,\n",
        "min_lift = 3, min_length = 2,max_length = 3)\n",
        "model_table=list(model)\n",
        "pd.DataFrame(model_table)"
      ],
      "metadata": {
        "id": "etb8n8o_Diwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LINEAR REGRESSION\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split from\n",
        "sklearn.linear_model import LinearRegression from\n",
        "sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "data=pd.read_csv('sales_A1.csv')\n",
        "data.info()\n",
        "data.describe()\n",
        "x= np.array(data[['TV']])\n",
        "y= np.array(data[['Sales']])\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size =0.7, test_size = 0.3, random_state = 100)\n",
        "x_train.shape\n",
        "x_test.shape\n",
        "model = LinearRegression()\n",
        "model.fit(x_train, y_train)\n",
        "model.coef_\n",
        "model.intercept_\n",
        "y_pred=model.predict(x_test)\n",
        "#Finding the value of coefficient of Determination\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "QoYp2TtEDjz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOGISTIC REGRESSION\n",
        "\n",
        "data=pd.read_csv('C:\\\\User_Dataset.csv')\n",
        "data.describe()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
        "test_size = 0.25, random_state = 25)\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "#Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)\n",
        "Write accuracy and confusion matrix\n"
      ],
      "metadata": {
        "id": "7sYgGKQjDyC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n"
      ],
      "metadata": {
        "id": "EWOOWFF4EDXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "zMABATgYEECW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\" Artificial intelligence is the future. Artificial\n",
        "intelligence is science fiction. Artificial intell\n"
      ],
      "metadata": {
        "id": "4_decwkMELzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "9UGYgCzUEOnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import heapq\n",
        "def clean_data(data):\n",
        "text = re.sub(r\"\\[[0-9]*\\]\",\" \",data)\n",
        "text = text.lower()\n",
        "text = re.sub(r'\\s+',\" \",text)\n",
        "text = re.sub(r\",\",\" \",text)\n",
        "return text\n",
        "cleaned_data = clean_data(text)\n",
        "\n",
        "\n",
        "sent_tokens = sent_tokenize(cleaned_data)\n",
        "word_tokens = word_tokenize(cleaned_data)\n",
        "word_frequency = {}\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "for word in word_tokens:\n",
        "if word not in stopwords:\n",
        "if word not in word_frequency.keys():\n",
        "word_frequency[word]=1\n",
        "else:\n",
        "word_frequency[word] +=1\n",
        "maximum_frequency = max(word_frequency.values())\n",
        "for word in word_frequency.keys():\n",
        "word_frequency[word] = (word_frequency[word]/maximum_frequency)\n",
        "\n",
        "\n",
        "sentences_score = {}\n",
        "for sentence in sent_tokens:\n",
        "for word in word_tokenize(sentence):\n",
        "if word in word_frequency.keys():\n",
        "if (len(sentence.split(\" \"))) <30:\n",
        "if sentence not in sentences_score.keys():\n",
        "sentences_score[sentence] = word_frequency[word]\n",
        "else:\n",
        "sentences_score[sentence] += word_frequency[word]\n",
        "\n",
        "\n",
        "def get_key(val):\n",
        "for key, value in sentences_score.items():\n",
        "if val == value:\n",
        "return key\n",
        "key = get_key(max(sentences_score.values()))\n",
        "summary = heapq.nlargest(5,sentences_score,key=sentences_score.get)\n",
        "print(\" \".join(summary))\n",
        "\n",
        "\n",
        "artificial intelligence is already part of our everyday lives. and all\n",
        "three are part of the reason why alphago trounced lee se-dol. all\n",
        "those statements are true it just depends on what flavor of ai you are\n",
        "referring to. artificial intelligence is science fiction. artificial\n",
        "intelligence is the future."
      ],
      "metadata": {
        "id": "cgJWJSg6EUhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neww\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n"
      ],
      "metadata": {
        "id": "-n9CEKiZEo8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"As the process of analyzing raw data to find trends and answer\n",
        "questions, the definition of data analyt"
      ],
      "metadata": {
        "id": "79l04s6HEsqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokens = sent_tokenize(text)\n",
        "word_tokens = word_tokenize(text)\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "frequency_distribution=FreqDist(word_tokens)\n",
        "print(frequency_distribution)\n",
        "frequency_distribution"
      ],
      "metadata": {
        "id": "R1T8hnO_Evc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "ax = plt.axes()\n",
        "frequency_distribution.plot(20,cumulative=False) #first 20 words starting from\n",
        "maximum onn Xaxis ax.set_title('Frequency Plot of words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oc7tTG3-Ez6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "0yBEQBofFPW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_cloud=WordCloud(background_color='black').generate(text)"
      ],
      "metadata": {
        "id": "lFGjf0StFSqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure()\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vWC88BPMFVtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newwwww\n",
        "nltk.download('vader_lexicon')\n"
      ],
      "metadata": {
        "id": "pg7rpuWYFX8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "vader_analyzer=SentimentIntensityAnalyzer()\n"
      ],
      "metadata": {
        "id": "NRGRb4zHFdUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1=\"I purchased headphones online. I am very happy with the product.\"\n",
        "print(vader_analyzer.polarity_scores(text1))\n"
      ],
      "metadata": {
        "id": "7KLmj_CyFfVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2=\"I saw the movie yesterday. The animation was really good but\n",
        "the script was ok.\" print(vader_analyzer.polarity_scores(text2))\n"
      ],
      "metadata": {
        "id": "c2ZWz7PyFiav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"As the process of analyzing raw data to find trends and answer questions, the definition of\n",
        "data analyt"
      ],
      "metadata": {
        "id": "aDd3g8AyFloa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sent_tokens = sent_tokenize(text)\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "\n",
        "print(\"Tokenized Sentences : \\n\", sent_tokens, \"\\n\") #break paragraph into sentences (.\n",
        "full stop is considere print(\"Tokenized Words : \\n\",word_tokens, \"\\n\") \n",
        "#break sentences into words ( <spaces are considered > )"
      ],
      "metadata": {
        "id": "AVOORa4-Ftas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "print(stopwords)\n"
      ],
      "metadata": {
        "id": "9P_s-7QyF5_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words_list=[]\n",
        "for words in word_tokens:\n",
        "if words not in stopwords:\n",
        "filtered_words_list.append(words)\n",
        "print(\"Tokenized Words : \\n\",word_tokens,\"\\n\")\n",
        "print(\"Filtered Words : \\n\",filtered_words_list,\"\\n\")\n"
      ],
      "metadata": {
        "id": "BdoShO3cF660"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming change writing, wrote, written can stemmed or reduced as write\n",
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "stemmed_text_words=[]\n",
        "for words in filtered_words_list:\n",
        "stemmed_text_words.append(porter_stemmer.stem(words))\n",
        "print(\"Filtered Words : \\n\",word_tokens,\"\\n\")\n",
        "print(\"Stemmed Words : \\n\",stemmed_text_words,\"\\n\")\n"
      ],
      "metadata": {
        "id": "VFCtj2uhF9hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "PorterStemmer().stem('eaten')\n"
      ],
      "metadata": {
        "id": "Kc84kUX0GBUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PorterStemmer().stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "ZUAFBs-LGGiD",
        "outputId": "8017a84a-0f6a-48e8-85a8-0279d8221773"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-660e390fbe3f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eating'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'PorterStemmer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PorterStemmer().stem('eated')"
      ],
      "metadata": {
        "id": "CMCYpPDFGKPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "y-xYYEZ2GOYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "word_text=\"eaten\"\n",
        "print(\"Lemmatized Word :"
      ],
      "metadata": {
        "id": "8MlebjdAGUFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\",lemmatizer.lemmatize(word_text,\"v\"))\n"
      ],
      "metadata": {
        "id": "0E2aWDEzGXbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}